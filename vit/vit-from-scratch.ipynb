{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76089,"sourceType":"datasetVersion","datasetId":42895}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:1]:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-30T09:59:36.844856Z","iopub.execute_input":"2024-05-30T09:59:36.845155Z","iopub.status.idle":"2024-05-30T10:00:54.183299Z","shell.execute_reply.started":"2024-05-30T09:59:36.845131Z","shell.execute_reply":"2024-05-30T10:00:54.182406Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cifar10/readme.txt\n/kaggle/input/cifar10/cifar10/labels.txt\n/kaggle/input/cifar10/cifar10/test/airplane/6221_airplane.png\n/kaggle/input/cifar10/cifar10/test/horse/3009_horse.png\n/kaggle/input/cifar10/cifar10/test/truck/9692_truck.png\n/kaggle/input/cifar10/cifar10/test/automobile/4067_automobile.png\n/kaggle/input/cifar10/cifar10/test/ship/8546_ship.png\n/kaggle/input/cifar10/cifar10/test/dog/2915_dog.png\n/kaggle/input/cifar10/cifar10/test/bird/1043_bird.png\n/kaggle/input/cifar10/cifar10/test/frog/4119_frog.png\n/kaggle/input/cifar10/cifar10/test/cat/2585_cat.png\n/kaggle/input/cifar10/cifar10/test/deer/6012_deer.png\n/kaggle/input/cifar10/cifar10/train/airplane/29606_airplane.png\n/kaggle/input/cifar10/cifar10/train/horse/46129_horse.png\n/kaggle/input/cifar10/cifar10/train/truck/42362_truck.png\n/kaggle/input/cifar10/cifar10/train/automobile/19301_automobile.png\n/kaggle/input/cifar10/cifar10/train/ship/32578_ship.png\n/kaggle/input/cifar10/cifar10/train/dog/38840_dog.png\n/kaggle/input/cifar10/cifar10/train/bird/11221_bird.png\n/kaggle/input/cifar10/cifar10/train/frog/16390_frog.png\n/kaggle/input/cifar10/cifar10/train/cat/7498_cat.png\n/kaggle/input/cifar10/cifar10/train/cat/7498_cat.png\n/kaggle/input/cifar10/cifar10/train/deer/43650_deer.png\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image\nfrom typing import Tuple, Union","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:00:54.184803Z","iopub.execute_input":"2024-05-30T10:00:54.185094Z","iopub.status.idle":"2024-05-30T10:00:59.234011Z","shell.execute_reply.started":"2024-05-30T10:00:54.185069Z","shell.execute_reply":"2024-05-30T10:00:59.233207Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\nBS = 16\nPS = 16\nC = 3\nN = (IMG_SIZE**2)//PS**2\nD_MODEL = PS**2*C\nN_ENCODER_BLOCKS = 4\nN_HEADS = 4","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:00:59.235026Z","iopub.execute_input":"2024-05-30T10:00:59.235402Z","iopub.status.idle":"2024-05-30T10:00:59.240509Z","shell.execute_reply.started":"2024-05-30T10:00:59.235377Z","shell.execute_reply":"2024-05-30T10:00:59.239520Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# os.listdir()\nroot_path = \"/kaggle/input/cifar10/cifar10\"","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:00:59.242839Z","iopub.execute_input":"2024-05-30T10:00:59.243130Z","iopub.status.idle":"2024-05-30T10:00:59.255123Z","shell.execute_reply.started":"2024-05-30T10:00:59.243106Z","shell.execute_reply":"2024-05-30T10:00:59.254249Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"with Image.open(f\"{root_path}/train/airplane/29606_airplane.png\") as img:\n    image_tensor = torch.tensor(np.asarray(img))\n    print(image_tensor.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:00:59.256388Z","iopub.execute_input":"2024-05-30T10:00:59.257030Z","iopub.status.idle":"2024-05-30T10:00:59.308136Z","shell.execute_reply.started":"2024-05-30T10:00:59.256997Z","shell.execute_reply":"2024-05-30T10:00:59.307303Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([32, 32, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor()\n])\n\ntrain_ds = ImageFolder(root=f\"{root_path}/train\",\n                        transform=transform)\ntest_ds = ImageFolder(root=f\"{root_path}/test\",\n                     transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:00:59.309061Z","iopub.execute_input":"2024-05-30T10:00:59.309301Z","iopub.status.idle":"2024-05-30T10:01:09.334171Z","shell.execute_reply.started":"2024-05-30T10:00:59.309279Z","shell.execute_reply":"2024-05-30T10:01:09.333344Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_ds.class_to_idx, test_ds.class_to_idx","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.335211Z","iopub.execute_input":"2024-05-30T10:01:09.335461Z","iopub.status.idle":"2024-05-30T10:01:09.343172Z","shell.execute_reply.started":"2024-05-30T10:01:09.335439Z","shell.execute_reply":"2024-05-30T10:01:09.342320Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"({'airplane': 0,\n  'automobile': 1,\n  'bird': 2,\n  'cat': 3,\n  'deer': 4,\n  'dog': 5,\n  'frog': 6,\n  'horse': 7,\n  'ship': 8,\n  'truck': 9},\n {'airplane': 0,\n  'automobile': 1,\n  'bird': 2,\n  'cat': 3,\n  'deer': 4,\n  'dog': 5,\n  'frog': 6,\n  'horse': 7,\n  'ship': 8,\n  'truck': 9})"},"metadata":{}}]},{"cell_type":"code","source":"classes = train_ds.classes\n\ntrain_dl = DataLoader(dataset=train_ds,\n                     batch_size=BS,\n                     shuffle=True)\ntest_dl = DataLoader(dataset=test_ds,\n                    batch_size=BS,\n                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.344344Z","iopub.execute_input":"2024-05-30T10:01:09.344662Z","iopub.status.idle":"2024-05-30T10:01:09.352424Z","shell.execute_reply.started":"2024-05-30T10:01:09.344638Z","shell.execute_reply":"2024-05-30T10:01:09.351517Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Embeddings Layer","metadata":{}},{"cell_type":"code","source":"with Image.open(f\"{root_path}/train/airplane/29606_airplane.png\") as img:\n    sample_img = transform(img)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.353517Z","iopub.execute_input":"2024-05-30T10:01:09.353826Z","iopub.status.idle":"2024-05-30T10:01:09.388933Z","shell.execute_reply.started":"2024-05-30T10:01:09.353802Z","shell.execute_reply":"2024-05-30T10:01:09.388128Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sample_img.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.392057Z","iopub.execute_input":"2024-05-30T10:01:09.392312Z","iopub.status.idle":"2024-05-30T10:01:09.397738Z","shell.execute_reply.started":"2024-05-30T10:01:09.392290Z","shell.execute_reply":"2024-05-30T10:01:09.396862Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"markdown","source":"CxHxW -> Nx(PS^2xC)","metadata":{}},{"cell_type":"code","source":"N, PS**2*C","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.399007Z","iopub.execute_input":"2024-05-30T10:01:09.399377Z","iopub.status.idle":"2024-05-30T10:01:09.406806Z","shell.execute_reply.started":"2024-05-30T10:01:09.399344Z","shell.execute_reply":"2024-05-30T10:01:09.405976Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(196, 768)"},"metadata":{}}]},{"cell_type":"code","source":"patch_conv_layer = nn.Conv2d(in_channels=3,\n                            out_channels=D_MODEL,\n                            kernel_size=PS,\n                            stride=PS)\n\npatch_conv_layer","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.407769Z","iopub.execute_input":"2024-05-30T10:01:09.408031Z","iopub.status.idle":"2024-05-30T10:01:09.430834Z","shell.execute_reply.started":"2024-05-30T10:01:09.408009Z","shell.execute_reply":"2024-05-30T10:01:09.430095Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))"},"metadata":{}}]},{"cell_type":"code","source":"img_conv = patch_conv_layer(sample_img)\nimg_conv.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.431775Z","iopub.execute_input":"2024-05-30T10:01:09.432032Z","iopub.status.idle":"2024-05-30T10:01:09.503770Z","shell.execute_reply.started":"2024-05-30T10:01:09.432010Z","shell.execute_reply":"2024-05-30T10:01:09.502924Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"torch.Size([768, 14, 14])"},"metadata":{}}]},{"cell_type":"code","source":"nn.Flatten()(img_conv).permute((1, 0)).shape","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.504799Z","iopub.execute_input":"2024-05-30T10:01:09.505057Z","iopub.status.idle":"2024-05-30T10:01:09.515198Z","shell.execute_reply.started":"2024-05-30T10:01:09.505035Z","shell.execute_reply":"2024-05-30T10:01:09.514016Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([196, 768])"},"metadata":{}}]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    \n    def __init__(self,\n                channels: int=C,\n                d_model: int=D_MODEL,\n                patch_size: int=PS) -> None:\n        \n        super().__init__()\n        \n        self.conv_layer = nn.Conv2d(\n            in_channels=channels,\n            out_channels = d_model,\n            kernel_size = patch_size,\n            stride = patch_size\n        )\n        \n        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n    \n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \n        # image -> B, C, IMG_SIZE, IMG_SIZE\n        patch_embedding = self.flatten(self.conv_layer(image)).permute((0, 2, 1))\n        return patch_embedding\n\nclass Embedding(nn.Module):\n    \n    def __init__(self,\n                channels: int=C,\n                d_model: int=D_MODEL,\n                patch_size: int=PS,\n                num_patches: int=N) -> None:\n        \n        super().__init__()\n        self.num_patches = num_patches\n        self.patch_embedding_layer = PatchEmbedding(channels=channels, d_model=d_model, patch_size=patch_size)\n        self.class_token_embedding = nn.Parameter(\n            data = torch.randn(size=(1, 1, d_model)),\n            requires_grad = True\n        )\n        self.positional_embedding = nn.Parameter(\n            data = torch.randn(size=(1, num_patches+1, d_model)),\n            requires_grad = True\n        )\n    \n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \n        # image -> B, C, H, W\n        BS = image.shape[0]\n        embedding = torch.cat((self.class_token_embedding.expand(BS, 1, -1), self.patch_embedding_layer(image)), dim=1) + \\\n                    self.positional_embedding # B, N+1, PS^2*C\n        return embedding","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.516807Z","iopub.execute_input":"2024-05-30T10:01:09.517073Z","iopub.status.idle":"2024-05-30T10:01:09.528520Z","shell.execute_reply.started":"2024-05-30T10:01:09.517051Z","shell.execute_reply":"2024-05-30T10:01:09.527409Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"images, labels = next(iter(train_dl))\nimages.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.529797Z","iopub.execute_input":"2024-05-30T10:01:09.530240Z","iopub.status.idle":"2024-05-30T10:01:09.690401Z","shell.execute_reply.started":"2024-05-30T10:01:09.530208Z","shell.execute_reply":"2024-05-30T10:01:09.689504Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"embedding_layer = Embedding()\nembeddings  = embedding_layer(images)\nembeddings.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.869972Z","iopub.execute_input":"2024-05-30T10:01:09.870231Z","iopub.status.idle":"2024-05-30T10:01:09.917799Z","shell.execute_reply.started":"2024-05-30T10:01:09.870207Z","shell.execute_reply":"2024-05-30T10:01:09.916921Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(torch.Size([16, 197, 768]), torch.Size([16]))"},"metadata":{}}]},{"cell_type":"markdown","source":"## MHA Block","metadata":{}},{"cell_type":"code","source":"class MHABlock(nn.Module):\n    \n    def __init__(self, \n                d_model:int=D_MODEL,\n                n_heads:int=N_HEADS) -> None:\n        \n        super().__init__()\n        self.mha_layer = nn.MultiheadAttention(\n            embed_dim=d_model,\n            num_heads=n_heads,\n            batch_first=True,\n        )\n        self.ln_layer = nn.LayerNorm(normalized_shape=d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \n        attn_output, _ = self.mha_layer(x, x, x)\n        return self.ln_layer(x + attn_output)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.918869Z","iopub.execute_input":"2024-05-30T10:01:09.919130Z","iopub.status.idle":"2024-05-30T10:01:09.925450Z","shell.execute_reply.started":"2024-05-30T10:01:09.919107Z","shell.execute_reply":"2024-05-30T10:01:09.924539Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## MLP Block","metadata":{}},{"cell_type":"code","source":"class MLPBlock(nn.Module):\n    \n    def __init__(self,\n                d_model:int=D_MODEL) -> None:\n        \n        super().__init__()\n        self.mlp_sub_block = nn.Sequential(\n            nn.Linear(in_features=d_model, out_features=d_model*4),\n            nn.GELU(),\n            nn.Linear(in_features=d_model*4, out_features=d_model)\n        )\n        self.ln_layer = nn.LayerNorm(normalized_shape=d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \n        return self.ln_layer(x + self.mlp_sub_block(x))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.926673Z","iopub.execute_input":"2024-05-30T10:01:09.927304Z","iopub.status.idle":"2024-05-30T10:01:09.934322Z","shell.execute_reply.started":"2024-05-30T10:01:09.927281Z","shell.execute_reply":"2024-05-30T10:01:09.933495Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Encoder Block","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    \n    def __init__(self, \n                d_model:int=D_MODEL,\n                n_heads:int=N_HEADS) -> None:\n        \n        super().__init__()\n        self.mha_block = MHABlock(d_model=d_model, n_heads=n_heads)\n        self.mlp_block = MLPBlock(d_model=d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \n        return self.mlp_block(self.mha_block(x))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.935331Z","iopub.execute_input":"2024-05-30T10:01:09.935617Z","iopub.status.idle":"2024-05-30T10:01:09.943877Z","shell.execute_reply.started":"2024-05-30T10:01:09.935585Z","shell.execute_reply":"2024-05-30T10:01:09.943145Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, \n                n_encoder_blocks:int=N_ENCODER_BLOCKS,\n                d_model:int=D_MODEL,\n                n_heads:int=N_HEADS) -> None:\n        \n        super().__init__()\n        self.encoder_blocks = nn.ModuleList([EncoderBlock(d_model=d_model, n_heads=n_heads) for _ in range(n_encoder_blocks)])\n    \n    def forward(self, x:torch.Tensor) -> torch.Tensor:\n        \n        for block in self.encoder_blocks:\n            x = block(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.948375Z","iopub.execute_input":"2024-05-30T10:01:09.948710Z","iopub.status.idle":"2024-05-30T10:01:09.954890Z","shell.execute_reply.started":"2024-05-30T10:01:09.948687Z","shell.execute_reply":"2024-05-30T10:01:09.953903Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## ViT","metadata":{}},{"cell_type":"code","source":"class ViT(nn.Module):\n    \n    def __init__(self,\n                 channels: int=C,\n                 d_model: int=D_MODEL,\n                 patch_size: int=PS,\n                 num_patches: int=N,\n                 n_heads: int=N_HEADS,\n                 n_encoder_blocks: int=N_ENCODER_BLOCKS,\n                 n_classes: int=len(classes)) -> None:\n        \n        \n        super().__init__()\n        \n        self.embedding_layer = Embedding(channels=C,\n                                      d_model=d_model,\n                                      patch_size=patch_size,\n                                      num_patches=num_patches)\n        self.encoder = Encoder(n_encoder_blocks=n_encoder_blocks,\n                              d_model=d_model,\n                              n_heads=n_heads)\n        self.classification_layer = nn.Linear(in_features=d_model, out_features=n_classes)\n    \n    def forward(self,\n               image: torch.Tensor,\n               label: torch.Tensor=None) -> Tuple[torch.Tensor]:\n        \n        embeddings = self.embedding_layer(image)\n        encoder_output = self.encoder(embeddings) # B, NUM_PATCHES+1, D_MODEL\n        logits = self.classification_layer(encoder_output[:, 0, :]) # B, NUM_CLASSES\n        \n        loss = None\n        if label is not None:\n            loss = F.cross_entropy(logits, label)\n        \n        return logits, loss","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.956061Z","iopub.execute_input":"2024-05-30T10:01:09.956376Z","iopub.status.idle":"2024-05-30T10:01:09.967730Z","shell.execute_reply.started":"2024-05-30T10:01:09.956346Z","shell.execute_reply":"2024-05-30T10:01:09.966915Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.968790Z","iopub.execute_input":"2024-05-30T10:01:09.969091Z","iopub.status.idle":"2024-05-30T10:01:09.977812Z","shell.execute_reply.started":"2024-05-30T10:01:09.969062Z","shell.execute_reply":"2024-05-30T10:01:09.976979Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"vit = ViT().to(device)\noptimizer = torch.optim.AdamW(params=vit.parameters(),\n                             lr=3e-4,\n                             weight_decay=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:09.978877Z","iopub.execute_input":"2024-05-30T10:01:09.979127Z","iopub.status.idle":"2024-05-30T10:01:10.486628Z","shell.execute_reply.started":"2024-05-30T10:01:09.979106Z","shell.execute_reply":"2024-05-30T10:01:10.485612Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def train_step() -> Tuple[float]:\n    \n    vit.train()\n    total_loss = 0\n    n_correct = 0\n    n_total = 0\n    for image, label in train_dl:\n        image, label = image.to(device), label.to(device)\n        logits, loss = vit(image, label)\n        n_correct += (torch.argmax(logits, dim=-1) == label).float().sum(0).item()\n        n_total += image.shape[0]\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss/len(train_dl), n_correct/n_total\n\n@torch.inference_mode()\ndef eval_step() -> Tuple[float]:\n    \n    vit.eval()\n    total_loss = 0\n    n_correct = 0\n    n_total = 0\n    for image, label in test_dl:\n        image, label = image.to(device), label.to(device)\n        logits, loss = vit(image, label)\n        n_correct += (torch.argmax(logits, dim=-1) == label).float().sum(0).item()\n        n_total += image.shape[0]\n        total_loss += loss.item()\n    \n    return total_loss/len(test_dl), n_correct/n_total","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:10.487936Z","iopub.execute_input":"2024-05-30T10:01:10.488305Z","iopub.status.idle":"2024-05-30T10:01:10.499314Z","shell.execute_reply.started":"2024-05-30T10:01:10.488270Z","shell.execute_reply":"2024-05-30T10:01:10.498357Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def train(epochs: int=2) -> None:\n    \n    vit.train()\n    for epoch in range(1, epochs+1):\n        \n        train_loss, train_acc = train_step()\n        eval_loss, eval_acc = eval_step()\n        print(f\"\"\"\nEpoch: {epoch}/{epochs}\n              train_loss: {train_loss:.4f} train_acc: {train_acc:.4f}\n              eval_loss:  {eval_loss:.4f}  eval_acc:  {eval_acc:.4f}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:01:10.500350Z","iopub.execute_input":"2024-05-30T10:01:10.500637Z","iopub.status.idle":"2024-05-30T10:01:10.509512Z","shell.execute_reply.started":"2024-05-30T10:01:10.500613Z","shell.execute_reply":"2024-05-30T10:01:10.508595Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}